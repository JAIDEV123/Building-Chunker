<!DOCTYPE html>
<html>
<head>

	<title>Experiments</title>
	<meta charset="utf-8">
	
	<!-- Custom Styles for website -->
	<link rel="stylesheet" href="./static/css/experiment.css">


	<!-- Standard Imported Bootstrap and More -->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
	<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

	<script type="text/javascript" src="./static/js/particles.min.js"></script>

</head>
<body>

	<div class="container-fluid h-10">
		<div id="particles-js"></div>
	</div>

	<nav class="navbar navbar-expand-lg main-nav header">
	  <a href="./index.html" class="logo">
	  	<h1><strong>NLP@IIIT-H</strong></h1>
	  </a>
	  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggler" aria-controls="navbarToggler" aria-expanded="false" aria-label="Toggle navigation">
	    <i class="fas fa-bars"></i>
	  </button>

	  <div class="collapse navbar-collapse" id="navbarToggler">
	    <ul class="navbar-nav ml-auto">
	      <li class="nav-item">
	        <a class="nav-link" href="#about">About</a>
	      </li>
	      <li class="nav-item">
	        <a class="nav-link" href="experiments.html">Experiments</a>
	      </li>      
	    </ul>
	  </div>
	</nav>

	<nav class="navbar navbar-expand-lg navbar-light">
	  <div class="w-100 text-center align-self-center" id="navbarToggler">
	    <ul class="navbar-nav ml-auto nav-center">
	      <li class="nav-item">
	        <a class="nav-link" href="#about">Introduction</a>
	      </li>
	      <li class="nav-item">
	        <a class="nav-link" href="experiments.html">Theory</a>
	      </li>
	      <li class="nav-item">
	        <a class="nav-link" href="experiments.html">Objective</a>
	      </li>
	      <li class="nav-item">
	        <a class="nav-link" href="experiments.html">Experiment</a>
	      </li> 
	      <li class="nav-item">
	        <a class="nav-link" href="experiments.html">Quizzes</a>
	      </li>
	      <li class="nav-item">
	        <a class="nav-link" href="experiments.html">Procedure</a>
	      </li>    
	    </ul>
	  </div>
	</nav>

	
	<div class="container-fluid experiment h-25">
		<div class="row h-100">
			<div class="col align-self-center text-center">
				<h1 class="txt-blue bold">
					Building Chunker
				</h1>
			</div>
		</div>
	</div>

	<div class="container blocko">
		
		<div class="row" >

			<div class="col w-100">
				<div class="container-fluid h-100">

					<!-- Introduction -->
					<div class="row blocko">
						<div class="col text-justify">
							<h1>Introduction</h1>
							<br>
							<p>Chunking is an analysis of a sentence which identifies the constituents (noun groups, verbs, verb groups, etc.) which are correlated. These are non-overlapping regions of text. Usually, each chunk contains a head, with the possible addition of some function words and modifiers either before or after depending on languages. These are non-recursive in nature i.e. a chunk cannot contain another chunk of the same category.</p>

							<p>
								Some of the groups possible are:
								<ol>
									<li>Noun Verb</li>
									<li>Verb Group</li>
								</ol>
							</p>

							<p>
								For example, the sentence 'He reckons the current account deficit will narrow to only 1.8 billion in September.' can be divided as follows:
							</p>

							<p class="code">[NP He ] [VP reckons ] [NP the current account deficit ] [VP will narrow ] [PP to ] [NP only 1.8 billion ] [PP in ] [NP September ]</p>

							<p>
								Each chunk has an open boundary and close boundary that delimit the word groups as a minimal non-recursive unit.
							</p>
						</div>
					</div>

					<div class="row blocko">
						<div class="col text-justify">
							<h1>Theory</h1>
							<br>
							<p>
								<h4>Hidden Markov Model</h4>
							</p>
							<p>
								In the mid 1980s, researchers in Europe began to use Hidden Markov models (HMMs) to disambiguate parts of speech. HMMs involve counting cases, and making a table of the probabilities of certain sequences. For example, once you've seen an article such as 'the', perhaps the next word is a noun 40% of the time, an adjective 40%, and a number 20%. Knowing this, a program can decide that "can" in "the can" is far more likely to be a noun than a verb or a modal. The same method can of course be used to benefit from knowledge about following words.
							</p>
							<p>
								More advanced ("higher order") HMMs learn the probabilities not only of pairs, but triples or even larger sequences. So, for example, if you've just seen an article and a verb, the next item may be very likely a preposition, article, or noun, but much less likely another verb.
							</p>
							<p>
								When several ambiguous words occur together, the possibilities multiply. However, it is easy to enumerate every combination and to assign a relative probability to each one, by multiplying together the probabilities of each choice in turn.
							</p>
							<p>
								It is worth remembering, as Eugene Charniak points out in Statistical techniques for natural language parsing, that merely assigning the most common tag to each known word and the tag "proper noun" to all unknowns, will approach 90% accuracy because many words are unambiguous.
							</p>
							<p>
								HMMs underlie the functioning of stochastic taggers and are used in various algorithms. Accuracies for one such algorithm (TnT) on various training data is shown here.
							</p>

							<h4>Conditional Random Field</h4>
							<p>
								Conditional random fields (CRFs) are a class of statistical modelling method often applied in machine learning, where they are used for structured prediction. Whereas an ordinary classifier predicts a label for a single sample without regard to "neighboring" samples, a CRF can take context into account. Since it can consider context, therefore CRF can be used in Natural Language Processing. Hence, Parts of Speech tagging is also possible. It predicts the POS using the lexicons as the context.
							</p>
							<p>
								In this experiment both algorithms are used for training and testing data. As the size of training corpus increases, it is observed that accuracy increases. Further, even features also play an important role for better output. In this experiment, we can see that Parts of Speech as a feature performs better than only lexicon as the feature. Therefore, it is important to select proper features for training a model to have better accuracy. 
							</p>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>

	<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
	<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
	<script type="text/javascript" src="./static/js/particles.js"></script>
</body>
</html>